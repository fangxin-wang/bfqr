{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1383f91aa7e43953"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import random\n",
    "import pyreadr\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k_max = 20\n",
    "k = k_max\n",
    "alpha = 0.1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1467bbd871ac8e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_data( n = 100000):\n",
    "  X = np.random.exponential( scale=1.0, size = (n,10) )\n",
    "  A = np.random.choice([0,1,2], size=n, p=[.1,.2,.7])\n",
    "  Y = np.sum(X, axis = 1) + A + 5*np.random.rand(n) #######\n",
    "  Y[A==1] = 10*np.random.rand(len(Y[A==1])) \n",
    "  Y = Y * np.random.rand(n) \n",
    "\n",
    "  df = pd.DataFrame(X, columns = [\"x_\"+str(_) for _ in range(10)] )\n",
    "  df[\"a\"] = pd.Series(A)\n",
    "  df[\"y\"] = pd.Series(Y)\n",
    "\n",
    "  return df\n",
    "\n",
    "df = generate_data( n = 100000)\n",
    "xfeatures = [\"x_\"+str(_) for _ in range(10)]+[\"a\"]\n",
    "yfeatures = \"y\"\n",
    "protected_features = \"a\"\n",
    "a_unique = [0,1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84a1afc1cfee8cce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## quantile regression: subject to any change of base model\n",
    "def linear_quantile(X_train, y_train, X_calib, X_test, quantiles = [0.05, 0.95]):\n",
    "    X_calib = X_calib.copy()\n",
    "    X_test = X_test.copy()\n",
    "    \n",
    "    quantreg = sm.QuantReg(y_train, X_train)  # fit linear quantile model\n",
    "    \n",
    "    \n",
    "    for q in quantiles:\n",
    "        X_calib.loc[:,'qt_pred_' +str(q)] = quantreg.fit(q=q).predict(X_calib.loc[:,xfeatures])\n",
    "        X_test.loc[:,'qt_pred_' +str(q)] = quantreg.fit(q=q).predict(X_test.loc[:,xfeatures])\n",
    "    \n",
    "    return X_calib, X_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "257957e327515db1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_max_distance_mean(agg_l):\n",
    "  a_num = len( a_unique )\n",
    "  n = int(len(agg_l )/ a_num)\n",
    "  bin_diff_l = []\n",
    "  for i in range(n):\n",
    "    max_gp, min_gp = float('-inf'), float('inf')\n",
    "    gp_l = agg_l[a_num*i: a_num*(i+1)]\n",
    "    for j in gp_l:\n",
    "      if j>max_gp:\n",
    "        max_gp = j\n",
    "      if j<min_gp:\n",
    "        min_gp = j\n",
    "    gap = abs(max_gp - min_gp)\n",
    "    gap = gap if gap <  float('inf') else 0\n",
    "    bin_diff_l.append(  gap  )\n",
    "  d_eop_1 = np.mean(bin_diff_l) * 100\n",
    "  return d_eop_1   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d54db5efc212e544"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_MMD_distance(df):\n",
    "  \n",
    "  from sdcit.sdcit_mod import SDCIT\n",
    "  from sdcit.utils import rbf_kernel_median\n",
    "  \n",
    "  df = df.dropna(axis=0)\n",
    "  X = np.array(df[\"if_cover\"]).reshape(-1,1)\n",
    "  Y = np.array(df[protected_features]).reshape(-1,1)\n",
    "  Z = np.array(df[yfeatures]).reshape(-1,1)\n",
    "  Kx, Ky, Kz = rbf_kernel_median(X,Y,Z) \n",
    "  test_statistic, p_value = SDCIT(Kx, Ky, Kz)\n",
    "  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "706c01a64f08fe59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from more_itertools import distinct_permutations as idp\n",
    "from itertools import permutations\n",
    "\n",
    "def get_unbiased(xi,yi,yj, v1,v2):\n",
    "  b1 = 1 if xi==v1 and yi==v2 else 0\n",
    "  b2 = 1 if xi==v1 else 0\n",
    "  b3 = 1 if yj==v2 else 0\n",
    "  return b1-b2*b3\n",
    "\n",
    "def get_4_unbiased(sample4):\n",
    "  perm = list( permutations([0,1,2,3]) )\n",
    "  h= 0\n",
    "  for i in perm:\n",
    "    for a in a_unique:\n",
    "      for if_cover in [False,True]:\n",
    "\n",
    "        #print(get_unbiased( sample4[i[0]][0], sample4[i[1]][0],sample4[i[1]][1], a, if_cover))\n",
    "        h +=  get_unbiased( sample4[i[0]][0], sample4[i[1]][0], sample4[i[1]][1], a, if_cover)\\\n",
    "              * get_unbiased( sample4[i[2]][0], sample4[i[3]][0], sample4[i[3]][1], a, if_cover) \n",
    "  return h/ len(perm)\n",
    "  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40eb2bb4f600e563"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Very slow\n",
    "def get_unbiased_T(test_df):\n",
    "  n = len(test_df)\n",
    "  poi_len = n+1\n",
    "  while poi_len >= n: \n",
    "    poi_len = np.random.poisson(lam = int(n/2) )\n",
    "  rows_id = random.sample( range(0,n-1), poi_len)\n",
    "  selected_data = test_df.iloc[rows_id, :]\n",
    "\n",
    "  out, bins = pd.qcut(selected_data[yfeatures], q = int(np.power(len(df), 0.4)), retbins=True , duplicates='drop')\n",
    "  selected_data[\"y_bin\"] = out\n",
    "  selected_data = selected_data[[protected_features, \"if_cover\" ,\"y_bin\"]]\n",
    "  selected_data.y_bin = pd.Categorical(selected_data.y_bin)\n",
    "  selected_data['code'] = selected_data.y_bin.cat.codes\n",
    "  selected_data = selected_data[[protected_features,\"if_cover\",\"code\"]]\n",
    "  \n",
    "  T_list = []\n",
    "  cnt = 0\n",
    "  for idx,gp_df in selected_data.groupby(\"code\"):\n",
    "    gp_num = len(gp_df)\n",
    "    cnt += 1\n",
    "    if gp_num <=4:\n",
    "      continue\n",
    "    U_list = []\n",
    "    total_v = 0\n",
    "    estimation_data = np.array( gp_df[[protected_features,\"if_cover\"]] )\n",
    "    estimation_data_idx = list(np.arange(len(estimation_data)))\n",
    "    for permu_4 in idp( estimation_data_idx , 4):\n",
    "      sample4 = estimation_data[permu_4,:]\n",
    "      #print(sample4)\n",
    "      res = get_4_unbiased(sample4)\n",
    "      U_list.append ( res )\n",
    "    U = np.mean(U_list)\n",
    "    #print(cnt,\"has\",gp_num,\"==>\",U)\n",
    "    T_list.append(gp_num*U)\n",
    "    #print( np.sum(T_list) )\n",
    "  return np.sum(T_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b338a1487e11890"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def get_unbiased_V(test_df): \n",
    "  round_T = []\n",
    "  for round in range(10):\n",
    "      n = len(test_df)\n",
    "      poi_len = n+1\n",
    "      while poi_len >= n: \n",
    "        poi_len = np.random.poisson(lam = int(n/2) )\n",
    "      rows_id = random.sample( range(0,n-1), poi_len)\n",
    "      selected_data = test_df.iloc[rows_id, :]\n",
    "\n",
    "      out, bins = pd.qcut(selected_data[yfeatures], q = int(np.power(len(df), 0.4)), retbins=True , duplicates='drop')\n",
    "      selected_data[\"y_bin\"] = out\n",
    "      selected_data = selected_data[[protected_features, \"if_cover\" ,\"y_bin\"]]\n",
    "      selected_data.y_bin = pd.Categorical(selected_data.y_bin)\n",
    "      selected_data['code'] = selected_data.y_bin.cat.codes\n",
    "      selected_data = selected_data[[protected_features,\"if_cover\",\"code\"]]\n",
    "      \n",
    "      distance_list = []\n",
    "      for idx,gp_df in selected_data.groupby(\"code\"):\n",
    "    \n",
    "        total_v = 0\n",
    "    \n",
    "        n = len(gp_df)\n",
    "        if n<4:\n",
    "            continue\n",
    "    \n",
    "        one_hot_feat = pd.get_dummies( gp_df [protected_features])\n",
    "        a_array, b_array = np.array( gp_df [\"if_cover\"]).reshape(-1,1) , np.array( one_hot_feat )\n",
    "        A,B = cdist(a_array, a_array, metric='cityblock'), cdist(b_array, b_array, metric='cityblock')\n",
    "        b_sum_all, b_row, b_col = np.sum(B,axis=None) , np.sum(B, axis =1 ) , np.sum(B , axis = 0 )\n",
    "        B_col, B_row = np.tile(b_col,(n,1)) , np.tile(b_row,(n,1)).T\n",
    "        \n",
    "        total_v = ( np.sum(A*B) \n",
    "          + np.sum( A * (b_sum_all - 2*B_col - 2*B_row + 2 * B) )/ ((n-2)*(n-3)) \n",
    "          - 2 * np.sum(A * (B_row- B))/ (n-2) )/( n* (n-1) )\n",
    "        distance_list.append( total_v * n )\n",
    "          \n",
    "      round_T.append(np.nansum(distance_list))\n",
    "  return np.mean(round_T)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4019a08b1664fc7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_fairness(eval_df):\n",
    "  \n",
    "  # 1. Coverage Rate of groups\n",
    "  coverage_gp = eval_df[[protected_features,\"if_cover\"]].groupby(protected_features).mean()[\"if_cover\"].tolist()\n",
    "  d_ec = ((np.array(coverage_gp) - (1-alpha))*100).tolist()\n",
    "  \n",
    "  # 2 EOC Test\n",
    "  # 2.1 mean max gap\n",
    "  \n",
    "  agg_df = eval_df.groupby([\"y_bin\",protected_features])['if_cover'].agg(['mean'])\n",
    "  agg_l = agg_df[\"mean\"].tolist()\n",
    "  d_eop_1 = get_max_distance_mean(agg_l)\n",
    "  \n",
    "  # 2.2. T \n",
    "  condi_tv = get_unbiased_V(eval_df)\n",
    "  #condi_tv = get_unbiased_T(eval_df)\n",
    "  print(\"condi_tv\", condi_tv)\n",
    "  d_eop = (d_eop_1,condi_tv)\n",
    "  \n",
    "  # 3. Average Width\n",
    "  interval_len = np.mean(eval_df['qt_pred_0.95'] - eval_df['qt_pred_0.05'])\n",
    "  \n",
    "  #print(\"{:.3f}%\".format(d_eop))\n",
    "  return d_ec,d_eop,interval_len\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a5d72f8e9a45a26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_split(seed):\n",
    "  global k\n",
    "  \n",
    "  # train: calib: test = 0.6: 0.2: 0.2\n",
    "  tmp, test = train_test_split(df,train_size=0.8, shuffle=True, random_state=seed)\n",
    "\n",
    "  # train: calib: test = 0.6: 0.2: 0.2\n",
    "  train, calib = train_test_split(tmp,train_size=0.75, shuffle=True, random_state=seed)\n",
    "\n",
    "  X_train,y_train = train.loc[:,xfeatures], train[yfeatures]\n",
    "\n",
    "  # run qr first; do not leak bin \n",
    "  calib_qr, test_qr = linear_quantile(X_train,y_train, calib, test,[0.05, 0.95])\n",
    "  #calib_qr, test_qr = nn_quantile(X_train,y_train, calib, test,[0.05, 0.95])\n",
    "\n",
    "  # bin cali into k sets\n",
    "  \n",
    "  out, bins = pd.qcut(calib[yfeatures].sort_values( ascending = True), k, retbins=True , duplicates='drop')\n",
    " \n",
    "  k = len(bins) - 1\n",
    "  y_bin_calib = out\n",
    "\n",
    "  # fit bin to test\n",
    "  y_bin_test = pd.cut(test[yfeatures], bins=bins, include_lowest=True)\n",
    "\n",
    "  # devide into X,y\n",
    "  X_calib,y_calib = calib.loc[:,xfeatures ], calib[yfeatures]\n",
    "  X_test,y_test = test.loc[:,xfeatures ], test[yfeatures]\n",
    "\n",
    "  # evaluate orginal performace\n",
    "  compare_df = test_qr\n",
    "  compare_df['if_cover'] = compare_df.apply(lambda x :  (x[yfeatures] >= x['qt_pred_0.05'])&(x[yfeatures] <= x['qt_pred_0.95']), axis = 1)\n",
    "  coverage_rate = compare_df['if_cover'].mean()\n",
    "  compare_df[\"y_bin\"] = y_bin_test\n",
    "  d_ec,d_eop,interval_len = evaluate_fairness(compare_df)\n",
    "  test_qr[\"y_bin\"], calib_qr[\"y_bin\"] = y_bin_test, y_bin_calib\n",
    "\n",
    "  return coverage_rate, d_ec,d_eop, interval_len, calib_qr, X_calib,y_calib, y_bin_calib , test_qr, X_test,y_test,y_bin_test\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ba63a753515f5fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_vanilla_cp(y_calib,calib_qr,test_qr,y_test,X_test,y_bin_test):\n",
    "\n",
    "  cal_labels = y_calib\n",
    "  cal_upper,val_upper = calib_qr[\"qt_pred_0.95\"],test_qr[\"qt_pred_0.95\"]\n",
    "  cal_lower,val_lower = calib_qr[\"qt_pred_0.05\"],test_qr[\"qt_pred_0.05\"]\n",
    "  \n",
    "  n = len(y_calib)\n",
    "\n",
    "  # Get scores. cal_upper.shape[0] == cal_lower.shape[0] == n\n",
    "  cal_scores = np.maximum(cal_labels-cal_upper, cal_lower-cal_labels)\n",
    "  # Get the score quantile\n",
    "  qhat = np.quantile(cal_scores, np.ceil((n+1)*(1-alpha))/n, interpolation='higher')\n",
    "  # Deploy (output=lower and upper adjusted quantiles)\n",
    "  prediction_sets = [val_lower - qhat, val_upper + qhat]\n",
    "  \n",
    "  compare_df = pd.DataFrame({})\n",
    "  compare_df['qt_pred_0.05'] , compare_df['qt_pred_0.95'] = prediction_sets[0], prediction_sets[1]\n",
    "  compare_df[yfeatures], compare_df[protected_features] = y_test, X_test[ protected_features ]\n",
    "  compare_df['if_cover'] = compare_df.apply(lambda x :  (x[yfeatures] >= x['qt_pred_0.05'])&(x[yfeatures] <= x['qt_pred_0.95']), axis = 1)\n",
    "  coverage_rate = compare_df['if_cover'].mean()\n",
    "  compare_df[\"y_bin\"] = y_bin_test\n",
    "\n",
    "  d_ec,d_eop,interval_len = evaluate_fairness(compare_df)\n",
    "\n",
    "  return coverage_rate, d_ec,d_eop,interval_len"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9f746bb39a2ac2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_vanilla_alphas( y_bin_unique, calib_qr, y_bin_calib, y_calib):\n",
    "  alpha_dict = {}\n",
    "  i = 0\n",
    "\n",
    "  cal_labels = y_calib\n",
    "  cal_upper = calib_qr[\"qt_pred_0.95\"]\n",
    "  cal_lower = calib_qr[\"qt_pred_0.05\"]\n",
    "  \n",
    "  n = len(y_calib)\n",
    "\n",
    "  cal_scores = np.maximum(cal_labels-cal_upper, cal_lower-cal_labels)\n",
    "  qhat = np.quantile(cal_scores, np.ceil((n+1)*(1-alpha))/n, interpolation='higher')\n",
    "\n",
    "  prediction_sets = [cal_lower - qhat, cal_upper + qhat]\n",
    "\n",
    "  bin_unique = y_bin_calib.unique()\n",
    "  bin_unaffected = np.zeros(k)\n",
    "  #bin_none = np.zeros(k)\n",
    "  for j in range(len(bin_unique)):\n",
    "    bin = bin_unique[j]\n",
    "    for i in range(len(y_calib)):\n",
    "      res = get_intersection([bin.left, bin.right], [prediction_sets[0].iloc[i],prediction_sets[1].iloc[i] ])\n",
    "      if res == None or (res[0] == bin.left) & (res[1] == bin.right):\n",
    "        bin_unaffected[j] += 1\n",
    "\n",
    "  compare_df = pd.DataFrame({})\n",
    "  compare_df['qt_pred_0.05'] , compare_df['qt_pred_0.95'] = prediction_sets[0], prediction_sets[1]\n",
    "  compare_df[yfeatures], compare_df[protected_features] = y_calib, calib_qr[ protected_features ]\n",
    "  compare_df['if_cover'] = compare_df.apply(lambda x :  (x[yfeatures] >= x['qt_pred_0.05'])\\\n",
    "                  &(x[yfeatures] <= x['qt_pred_0.95']), axis = 1)\n",
    "  compare_df[\"y_bin\"] = y_bin_calib\n",
    "\n",
    "  alpha_l =  compare_df.groupby([\"y_bin\"])[\"if_cover\"].mean().tolist() \n",
    "  for i in range(k):\n",
    "      alpha_dict[y_bin_unique[i]] = alpha_l[i]\n",
    "  return alpha_dict,bin_unaffected"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "256fab2a5a0f406d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_Q_gp_dict(y_bin_unique,df_qt_dict,protected_f_unique, alpha_dict ):\n",
    "  Q_gp_dict = {}\n",
    "  for y_bin in y_bin_unique:\n",
    "    for protected_f in protected_f_unique:\n",
    "      res = df_qt_dict[ (protected_f,y_bin) ]\n",
    "      n_a_m = len(res)\n",
    "      if n_a_m:\n",
    "        Q_gp = np.quantile(  res , np.min( [1,np.max( [0,np.min([1,np.ceil((n_a_m + 1)*( alpha_dict[y_bin]))/n_a_m ])])]), interpolation='higher' )\n",
    "        #np.quantile(  res , np.max([0,np.min([1, alpha_dict[y_bin]])]) )\n",
    "        Q_gp_dict[(protected_f,y_bin)] = Q_gp\n",
    "      else:\n",
    "        Q_gp_dict[(protected_f,y_bin)] = 0\n",
    "  return Q_gp_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dc5095bb8f887a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_intersection(interval1, interval2):\n",
    "    new_min = max(interval1[0], interval2[0])\n",
    "    new_max = min(interval1[1], interval2[1])\n",
    "    return [new_min, new_max] if new_min <= new_max else None\n",
    "\n",
    "def find_interval(Q_gp_dict, x, y_bin_unique):\n",
    "  \n",
    "  int_len, M  = 0, len(y_bin_unique)\n",
    "  flag = 0\n",
    "  interval_intersection_dict = {}\n",
    "  first_bin, last_bin = None,None\n",
    "  l,r = None, None\n",
    "  for m in range(M):\n",
    "    bin_flag = 0\n",
    "    y_bin = y_bin_unique[m]\n",
    "    Q_gp = Q_gp_dict[(x[protected_features],y_bin)]\n",
    "    interval_intersection = get_intersection( [ x[\"qt_pred_0.05\"] - Q_gp, x[\"qt_pred_0.95\"] + Q_gp ] , [y_bin.left, y_bin.right] )\n",
    "    \n",
    "    if interval_intersection!= None:\n",
    "      if first_bin == None:\n",
    "        first_bin = y_bin\n",
    "        l = interval_intersection[0] \n",
    "      last_bin = y_bin\n",
    "      r = interval_intersection[1]\n",
    "      int_len += interval_intersection[1] - interval_intersection[0] \n",
    "      if (x[yfeatures]<=interval_intersection[1])&(x[yfeatures]>=interval_intersection[0]):\n",
    "        bin_flag, flag = 1, 1\n",
    "    interval_intersection_dict[y_bin] = (interval_intersection,bin_flag)\n",
    "\n",
    "  if r == None:\n",
    "    filled_len = 0\n",
    "    filled_flag = 0\n",
    "  else:\n",
    "    filled_len = r - l\n",
    "    filled_flag = 1 if (x[yfeatures]<= r )&(x[yfeatures]>= l) else 0\n",
    "\n",
    "  #print(\"int_len\",int_len,\"l\", l ,\"r\",r ,\"y\", x[yfeatures] ,\"y_lo\", x[\"qt_pred_0.05\"] ,\"y_hi\", x[\"qt_pred_0.95\"] )\n",
    "  return [int_len, flag, filled_len,filled_flag]\n",
    "\n",
    "def if_bin_affected( bin, current_beta, cal_upper, cal_lower,y_calib ):\n",
    "\n",
    "   if_lower_affected = current_beta >=  cal_lower - bin.right\n",
    "   if_upper_affected = current_beta >=  bin.left - cal_upper\n",
    "   return  if_upper_affected, if_lower_affected\n",
    "\n",
    "def get_min_max_m( a, bins, Q_gp_dict, upper, lower, max = True):\n",
    "  # if max == True, return the max m that has intersection\n",
    "  n_bins = len(bins)\n",
    "  l = range( n_bins ) if max == False else reversed( range( n_bins) )\n",
    "  for m in l:\n",
    "    bin = bins[m]\n",
    "    Q = Q_gp_dict[(a,bin)]\n",
    "    intersec = get_intersection( [bin.left,bin.right], [lower - Q, upper + Q] )\n",
    "    if  intersec!= None:\n",
    "      return m"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "419af9f2af5e7c2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_updated_list(m, a, current_beta, Q_gp_dict, bins ,cal_upper, cal_lower,y_calib, y_bin_calib, bin_unaffected):\n",
    "\n",
    "  bin = bins[m] \n",
    "  updated_beta = Q_gp_dict[(a,bin)] # new Q\n",
    "  idx = y_bin_calib == bin\n",
    "  upper, lower, y = cal_upper[idx], cal_lower[idx], y_calib[idx]\n",
    "  u_cur, l_cur = if_bin_affected( bin, current_beta, upper, lower, y)\n",
    "  u_new, l_new = if_bin_affected( bin, updated_beta, upper, lower, y)\n",
    "\n",
    "  # k-: Q decreses, intersection is possible to decrease, then true->false\n",
    "  if_decrease = current_beta > updated_beta\n",
    "  u_changed_idx = (u_cur == if_decrease) &  ( u_new !=  if_decrease) \n",
    "  l_changed_idx = (l_cur == if_decrease) &  ( l_new !=  if_decrease) \n",
    "  u_changed_n, l_changed_n = np.sum(u_changed_idx), np.sum(l_changed_idx)\n",
    "\n",
    "  flag = 1 if if_decrease else -1\n",
    "  len_diff = 0\n",
    "\n",
    "  bin_unaffected[m] += -flag * u_changed_n\n",
    "  bin_unaffected[m] += -flag * l_changed_n\n",
    "\n",
    "  for u_changed, l_changed in zip(upper[u_changed_idx], lower[l_changed_idx]):\n",
    "    m_u = get_min_max_m( a, bins, Q_gp_dict, u_changed, l_changed, max = True)\n",
    "    m_l = get_min_max_m( a, bins, Q_gp_dict, u_changed, l_changed, max = False)\n",
    "    bin_unaffected[m_u] += flag\n",
    "    bin_unaffected[m_l] += flag\n",
    "\n",
    "  return bin_unaffected"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95ecfd221953dd59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def opt_alpha_dict(alpha_dict,X_calib,protected_f_unique, df_qt_dict,bin_unaffected, cal_upper,cal_lower,y_calib, y_bin_calib,test_qr,test = False):\n",
    "\n",
    "  alpha_dict_updated = alpha_dict.copy()\n",
    "  #print(alpha_dict_updated)\n",
    "  bins = list( alpha_dict.keys() )\n",
    "  Q_gp_dict = get_Q_gp_dict(bins,df_qt_dict,protected_f_unique, alpha_dict_updated )\n",
    "  log = []\n",
    "\n",
    "  n = len(X_calib)\n",
    "\n",
    "  k_minus_array = np.zeros(k)\n",
    "  k_minus_array = np.array([np.inf]*k)\n",
    "  max_step = np.zeros( (2,len(protected_f_unique), k) )\n",
    "\n",
    "  total_step_for_run = 0\n",
    "\n",
    "  for round in range(1000):\n",
    "\n",
    "    # for each bin\n",
    "    k_plus_bins, k_minus_bins = np.zeros(k), np.zeros(k)\n",
    "\n",
    "    for m in range(k):\n",
    "      bin = bins[m]\n",
    "\n",
    "      # total min/max gradient of bin i\n",
    "      k_minus_gp , k_plus_gp = 0,0\n",
    "      for a in protected_f_unique:\n",
    "       \n",
    "        q_list = np.array( df_qt_dict[(a, bin)] )\n",
    "        n_a_m = len(q_list)\n",
    "        # update current beta\n",
    "        beta = alpha_dict_updated[bin]\n",
    "        \n",
    "        if n_a_m:\n",
    "          current_beta =  np.quantile ( q_list, np.min( [1,np.max( [0,np.min([1,np.ceil((n_a_m+1)*(beta))/n_a_m ])])]), interpolation='higher') \n",
    "        else:\n",
    "          current_beta = None  \n",
    "          max_step[0,a,m], max_step[1,a,m] = 1, 1\n",
    "          k_minus_gp += 0\n",
    "          k_plus_gp += 0\n",
    "          continue\n",
    "\n",
    "        #### minus\n",
    "        try:\n",
    "          if  max_step[0,a,m] <= 0:\n",
    "              max_step[0,a,m] = 1/n_a_m  \n",
    "          \n",
    "          last_beta = np.max( q_list[ q_list< current_beta ] )\n",
    "          k_minus = (current_beta-last_beta)\n",
    "          k_minus_gp += k_minus * (1 - bin_unaffected[m]/n  ) \n",
    "\n",
    "        except:\n",
    "          #print(\"no k_minus\")\n",
    "          k_minus_gp = - np.inf \n",
    "\n",
    "        #### plus\n",
    "        try:\n",
    "          if  max_step[1,a,m] <= 0:\n",
    "              max_step[1,a,m] = 1/n_a_m  \n",
    " \n",
    "          next_beta = np.min( q_list[ q_list> current_beta ] )\n",
    "          k_plus = (next_beta-last_beta)/2\n",
    "          k_plus_gp += k_plus * (1 - bin_unaffected[m]/n) \n",
    "        except:\n",
    "          #print(\"no k_plus\")\n",
    "          k_plus_gp = np.inf\n",
    "\n",
    "      k_plus_bins[m], k_minus_bins[m] = k_plus_gp, k_minus_gp #  k_plus_gp * (1 + epsilon), k_minus_gp * (1 - epsilon)   ############## test here\n",
    "\n",
    "    # for this round, greedily rise the beta in one bin and lower the same amount of beta in another \n",
    "    k_plus_min_bin_num, k_minus_max_bin_num  = np.argmin(k_plus_bins), np.argmax( k_minus_bins )\n",
    "    k_plus_min_bin,  k_minus_max_bin = k_plus_bins[k_plus_min_bin_num], k_minus_bins[k_minus_max_bin_num]\n",
    "\n",
    "    sub_r = 0\n",
    "    # note:  can not minus and plus the same bin\n",
    "    while (k_plus_min_bin_num == k_minus_max_bin_num):\n",
    "        k_plus_min_bin,  k_minus_max_bin = k_plus_bins[k_plus_min_bin_num], k_minus_bins[k_minus_max_bin_num]\n",
    "        if sub_r > 10:\n",
    "          break\n",
    "        sub_r += 1\n",
    "        arg_l = np.concatenate( (np.arange(0,k_plus_min_bin_num) , np.arange(k_plus_min_bin_num+1,k) ) )\n",
    "        k_minus_max_bin_new = k_minus_bins[ np.argmax( k_minus_bins[arg_l] ) ]\n",
    "        if k_minus_max_bin_new  > k_plus_min_bin:\n",
    "          k_minus_max_bin_num = np.argmax( k_minus_bins[arg_l] )\n",
    "        else:\n",
    "          arg_l = np.concatenate( (np.arange(0,k_minus_max_bin_num) , np.arange(k_minus_max_bin_num+1,k) ) )\n",
    "          k_plus_min_bin_num =  np.argmax( k_plus_bins[arg_l] )\n",
    "          \n",
    "    epsilon = 0.02\n",
    "    #print(\"===\",k_plus_min_bin,k_minus_max_bin)\n",
    "    #if k_plus_min_bin * (1 + epsilon) >= k_minus_max_bin * (1-epsilon):  ############## test here\n",
    "    if k_plus_min_bin + epsilon >= k_minus_max_bin: \n",
    "      break\n",
    "    # find max possible step\n",
    "    \n",
    "    total_possible_step = np.max([ 0, np.min( max_step[:,:,k_plus_min_bin_num] )] )\n",
    "    total_step_for_run += total_possible_step\n",
    "    \n",
    "    # update max_step \n",
    "    max_step[1,:,k_plus_min_bin_num] -= total_possible_step\n",
    "    max_step[0,:,k_minus_max_bin_num] -= total_possible_step\n",
    "\n",
    "    # update alpha dict for calibrated bin\n",
    "    alpha_dict_updated[bins[k_plus_min_bin_num]] += total_possible_step\n",
    "    alpha_dict_updated[bins[k_minus_max_bin_num]] -= total_possible_step\n",
    "\n",
    "    # update affected bin\n",
    "    Q_gp_dict_ori = Q_gp_dict\n",
    "    Q_gp_dict = get_Q_gp_dict(bins,df_qt_dict,protected_f_unique, alpha_dict_updated )\n",
    "    for a in protected_f_unique:\n",
    "      for m_updated in [k_plus_min_bin_num,k_minus_max_bin_num]:\n",
    "        idx = X_calib[protected_features] == a\n",
    "        bin_unaffected = get_updated_list(m_updated, a, Q_gp_dict_ori[(a,bins[m_updated])], Q_gp_dict, bins ,\\\n",
    "                                          cal_upper[idx], cal_lower[idx], y_calib[idx], y_bin_calib[idx], bin_unaffected)\n",
    "    if test and round%5==0:\n",
    "      # in training epoch, see if the test interval width decreases\n",
    "      X_to_use = test_qr[[ protected_features,\"qt_pred_0.05\",\t\"qt_pred_0.95\", yfeatures ,\"y_bin\"]]\n",
    "      res = X_to_use.apply(lambda x: find_interval(Q_gp_dict,x,bins), axis = 1)\n",
    "      X_to_use[\"int_len\"] = res.apply(lambda x: x[0])\n",
    "      X_to_use[\"if_cover\"] = res.apply(lambda x: x[1])\n",
    "      X_to_use[\"filled_len\"] = res.apply(lambda x: x[2])\n",
    "\n",
    "      len_mean, cover_mean, filled_mean = np.mean(X_to_use[\"int_len\"]), np.mean(X_to_use[\"if_cover\"]),np.mean(X_to_use[\"filled_len\"])\n",
    "      log.append([round,len_mean, cover_mean,filled_mean])\n",
    "\n",
    "  if not test:\n",
    "    print(round, total_step_for_run)\n",
    "    return alpha_dict_updated\n",
    "  else:\n",
    "    return log"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e237ca8eadb1e76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_my_bin_cp( X_calib, y_calib, y_bin_calib, calib_qr, X_test, y_test, test_qr, y_bin_test):\n",
    "\n",
    "  # calculate scores of calibration data\n",
    "  cal_labels = y_calib\n",
    "  cal_upper,val_upper = calib_qr[\"qt_pred_0.95\"],test_qr[\"qt_pred_0.95\"]\n",
    "  cal_lower,val_lower = calib_qr[\"qt_pred_0.05\"],test_qr[\"qt_pred_0.05\"]\n",
    "\n",
    "  cal_scores = np.maximum(cal_labels-cal_upper, cal_lower-cal_labels)\n",
    "\n",
    "  y_bin_unique = y_bin_calib.unique()\n",
    "  protected_f_unique = X_calib[protected_features].unique()\n",
    "\n",
    "  # store data by group to compute quantile faster\n",
    "  df_qt_dict = {}\n",
    "  df_qt_comp = pd.concat([X_calib[protected_features],y_bin_calib, cal_scores],axis = 1)\n",
    "  df_qt_comp.columns = [protected_features,\tyfeatures, \"score\"]\n",
    "  for a in protected_f_unique:\n",
    "    for y in y_bin_unique:\n",
    "      df_qt_dict[(a,y)] = df_qt_comp[(df_qt_comp[protected_features] == a) &\\\n",
    "                                     (df_qt_comp[yfeatures] == y)] [\"score\"].tolist()\n",
    "\n",
    "  # use the proportion of groups in calib set as a initial point\n",
    "\n",
    "  alpha_dict,bin_unaffected = get_vanilla_alphas( y_bin_unique, calib_qr, y_bin_calib, y_calib)\n",
    "\n",
    "  # opt\n",
    "  alpha_dict_updated = opt_alpha_dict(alpha_dict,X_calib,protected_f_unique,df_qt_dict,bin_unaffected, cal_upper,cal_lower,y_calib,y_bin_calib,test_qr)\n",
    "  #print( \"alpha_mean\",np.mean( list(alpha_dict_updated.values() )) )\n",
    "\n",
    "  # get Q for each bin according to alphas\n",
    "  Q_gp_dict = get_Q_gp_dict(y_bin_unique,df_qt_dict,protected_f_unique, alpha_dict_updated)\n",
    "\n",
    "  # apply Q to test data\n",
    "  X_to_use = test_qr[[ protected_features,\"qt_pred_0.05\",\t\"qt_pred_0.95\", yfeatures ,\"y_bin\"]]\n",
    "\n",
    "  res = X_to_use.apply(lambda x: find_interval(Q_gp_dict,x,y_bin_unique), axis = 1)\n",
    "  X_to_use[\"int_len\"] = res.apply(lambda x: x[0])\n",
    "  X_to_use[\"if_cover\"] = res.apply(lambda x: x[1])\n",
    "  #print(X_to_use.groupby([\"y_bin\",protected_features]).mean()[\"if_cover\"])\n",
    "\n",
    "  X_to_use[\"filled_len\"] = res.apply(lambda x: x[2])\n",
    "  X_to_use[\"filled_flag\"] = res.apply(lambda x: x[3])\n",
    "\n",
    "  # evaluate test result\n",
    "  # note interval_len compuation is in different way\n",
    "  d_ec,d_eop,interval_len = evaluate_fairness(X_to_use)\n",
    "  interval_len =  X_to_use[\"int_len\"].mean()\n",
    "  coverage_rate = X_to_use['if_cover'].mean()\n",
    "  print(coverage_rate, interval_len)\n",
    "\n",
    "  # evaluate filled intervals\n",
    "  X_filled = test_qr[[ protected_features,\"qt_pred_0.05\",\t\"qt_pred_0.95\", yfeatures ,\"y_bin\"]]\n",
    "  X_filled[\"int_len\"], X_filled[\"if_cover\"] = X_to_use[\"filled_len\"] , X_to_use[\"filled_flag\"]\n",
    "  d_ec_filled,d_eop_filled,interval_len_filled = evaluate_fairness(X_filled)\n",
    "  interval_len_filled = X_filled[\"int_len\"].mean()\n",
    "  coverage_rate_filled = X_filled[\"if_cover\"].mean()\n",
    "\n",
    "  return coverage_rate, d_ec,d_eop,interval_len,\\\n",
    "         coverage_rate_filled, d_ec_filled,d_eop_filled,interval_len_filled\n",
    "\n",
    "  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e80f1f8644cc731"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_bin_exp(seed):\n",
    "  # no calibration\n",
    "  c_origin, d_ec_origin, d_eop_origin, interval_len_origin, calib_qr, X_calib,y_calib, y_bin_calib , test_qr, X_test,y_test,y_bin_test = run_split(seed)\n",
    "  # my bin cp\n",
    "  \n",
    "  c_bin, d_ec_bin, d_eopc_bin, interval_len_bin, c_filled, d_ec_filled, d_eopc_filled, interval_len_filled = \\\n",
    "  run_my_bin_cp( X_calib, y_calib, y_bin_calib, calib_qr, X_test, y_test, test_qr, y_bin_test  )\n",
    "                                                         \n",
    "  return  c_origin, d_ec_origin, d_eop_origin, interval_len_origin, c_bin, d_ec_bin, d_eopc_bin, \\\n",
    "           interval_len_bin, c_filled, d_ec_filled, d_eopc_filled, interval_len_filled\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cb78d8649955b29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 0\n",
    "c_origin, d_ec_origin, d_eop_origin, interval_len_origin, c_bin, d_ec_bin, d_eopc_bin, \\\n",
    "           interval_len_bin, c_filled, d_ec_filled, d_eopc_filled, interval_len_filled = run_bin_exp(seed)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95058d9f04a9648c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4d1041b1056be6b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
